{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings( 'ignore' )\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # supress tensorflow warnings\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import All_UT_Models\n",
    "import UT_Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters for this model\n",
    "\n",
    "MAX_LEN = 100\n",
    "EMBED_SIZE = 300\n",
    "PRE_TRAINED_FLAG = True\n",
    "EMBED_TRAINABLE = False\n",
    "\n",
    "EMB_WEIGHTS_INIT = 'he_normal'\n",
    "SPDRPT = 0.4\n",
    "DRPT = 0.2\n",
    "FC_WEIGHTS_INIT = 'he_uniform'\n",
    "FC_ACT = 'elu'\n",
    "LR_RATE = 1e-3\n",
    "OPTIMIZER = 'adam'\n",
    "LSTM_UNITS = 100\n",
    "\n",
    "BATCH = 32\n",
    "NEPOCHS = 30\n",
    "PATIENCE = 5\n",
    "DECAY = True\n",
    "DECAY_RATE = 0.3\n",
    "DECAY_AFTER = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EMBEDDING_FILE = './Embeddings/urduvec_140M_100K_300d.txt'\n",
    "EMBEDDING_FILE = './Embeddings/cc.ur.300.vec'\n",
    "\n",
    "EMBEDDING_MATRIX = []\n",
    "\n",
    "modelname = 'BLSTM-ft'\n",
    "\n",
    "modelpath = './Saved Models/' + modelname + '/'\n",
    "modelresults = './Model Results'\n",
    "modelsummaries = './Model - Summaries-Figures'\n",
    "\n",
    "DIRECTORIES_TO_BE_CREATED = [ modelpath, modelresults, modelsummaries ]\n",
    "for directory in DIRECTORIES_TO_BE_CREATED:\n",
    "    if not os.path.exists( directory ):\n",
    "        os.makedirs( directory )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hms_string( sec_elapsed ):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{} hrs {:>02} mins {:>05.2f} secs\".format( h, m, s )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72771, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheets = [ pd.read_excel( 'RUT_UT_Comments.xlsx', 'Sheet1' ), pd.read_excel( 'RUT_UT_Comments.xlsx', 'Sheet2' ) ]\n",
    "df = pd.concat( sheets )\n",
    "df.reset_index( drop=True, inplace=True )\n",
    "\n",
    "df.Urdu = df.Urdu.astype( 'str' )\n",
    "df.Urdu = df.Urdu.apply( UT_Utils.urdu_preprocessing )\n",
    "\n",
    "df.dropna( inplace=True )\n",
    "df.reset_index( drop=True, inplace=True )\n",
    "\n",
    "del sheets\n",
    "gc.collect()\n",
    "\n",
    "xcolumn = 'Urdu'\n",
    "ycolumn = 'Toxic'\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words in Pretrained Embeddings:  1153999\n",
      "Pretrained Embeddings mean: -0.00027458658  std:  0.024344679\n"
     ]
    }
   ],
   "source": [
    "def get_coefs( word, *arr ):\n",
    "    return word, np.asarray( arr, dtype='float32' )\n",
    "\n",
    "def get_vectors( tokenizer, emb_mean, emb_std, embed_size ):\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    word_index = tokenizer.word_index\n",
    "    num_words = len( word_index ) + 1\n",
    "    embedding_matrix = np.random.normal( emb_mean, emb_std, ( num_words, embed_size ) ) # for unk tokens\n",
    "    embedding_matrix[0] = np.zeros( ( embed_size ) ) # replace zero padding with zeros\n",
    "    for word, i in word_index.items(  ):\n",
    "        embedding_vector = EMBEDDINGS_INDEX.get( word )\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[ i ] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "    print( \"Pretrained Embeddings: %d hits (%d misses)\" % ( hits, misses ) )\n",
    "    gc.collect()\n",
    "    return embedding_matrix\n",
    "\n",
    "if PRE_TRAINED_FLAG == True:\n",
    "    EMBEDDINGS_INDEX = dict( get_coefs( *o.strip().split() ) for o in open( EMBEDDING_FILE, encoding='utf-8' )  if len(o)>19 )\n",
    "    print( 'Total Words in Pretrained Embeddings: ', len( EMBEDDINGS_INDEX.values() ) )\n",
    "    all_embs = np.stack( EMBEDDINGS_INDEX.values() )\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    print( 'Pretrained Embeddings mean:', emb_mean, ' std: ', emb_std )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold( n_splits=5, random_state=0, shuffle=True )\n",
    "print(skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local System Time: 12:08 AM\n",
      "Pretrained Embeddings: 28830 hits (16914 misses)\n",
      "Epoch: 001 --LR: 1e-03 --MaxValF1: 0.7697368 --CurValF1: 0.7697368 --Patience: 01 --F1 improved: 0.7697368\n",
      "Epoch: 002 --LR: 1e-03 --MaxValF1: 0.7909408 --CurValF1: 0.7909408 --Patience: 01 --F1 improved: 0.7909408\n",
      "Epoch: 003 --LR: 1e-03 --MaxValF1: 0.8086093 --CurValF1: 0.8086093 --Patience: 01 --F1 improved: 0.8086093\n",
      "Epoch: 004 --LR: 1e-03 --MaxValF1: 0.8161740 --CurValF1: 0.8161740 --Patience: 01 --F1 improved: 0.8161740\n",
      "Epoch: 005 --LR: 1e-03 --MaxValF1: 0.8296445 --CurValF1: 0.8296445 --Patience: 01 --F1 improved: 0.8296445\n",
      "Epoch: 006 --LR: 1e-03 --MaxValF1: 0.8327954 --CurValF1: 0.8327954 --Patience: 01 --F1 improved: 0.8327954\n",
      "Epoch: 007 --LR: 1e-03 --MaxValF1: 0.8346667 --CurValF1: 0.8346667 --Patience: 01 --F1 improved: 0.8346667\n",
      "Epoch: 008 --LR: 1e-03 --MaxValF1: 0.8364362 --CurValF1: 0.8364362 --Patience: 01 --F1 improved: 0.8364362\n",
      "Epoch: 009 --LR: 1e-03 --MaxValF1: 0.8425865 --CurValF1: 0.8425865 --Patience: 01 --F1 improved: 0.8425865\n",
      "Epoch: 010 --LR: 1e-03 --MaxValF1: 0.8425865 --CurValF1: 0.8394816 --Patience: 01\n",
      "Epoch: 011 --LR: 3e-04 --MaxValF1: 0.8434224 --CurValF1: 0.8434224 --Patience: 02 --F1 improved: 0.8434224\n",
      "Epoch: 012 --LR: 3e-04 --MaxValF1: 0.8451548 --CurValF1: 0.8451548 --Patience: 01 --F1 improved: 0.8451548\n",
      "Epoch: 013 --LR: 3e-04 --MaxValF1: 0.8481183 --CurValF1: 0.8481183 --Patience: 01 --F1 improved: 0.8481183\n",
      "Epoch: 014 --LR: 3e-04 --MaxValF1: 0.8490124 --CurValF1: 0.8490124 --Patience: 01 --F1 improved: 0.8490124\n",
      "Epoch: 015 --LR: 3e-04 --MaxValF1: 0.8490124 --CurValF1: 0.8460259 --Patience: 01\n",
      "Epoch: 016 --LR: 9e-05 --MaxValF1: 0.8490124 --CurValF1: 0.8489496 --Patience: 02\n",
      "Epoch: 017 --LR: 3e-05 --MaxValF1: 0.8491547 --CurValF1: 0.8491547 --Patience: 03 --F1 improved: 0.8491547\n",
      "Epoch: 018 --LR: 3e-05 --MaxValF1: 0.8491547 --CurValF1: 0.8489162 --Patience: 01\n",
      "Epoch: 019 --LR: 8e-06 --MaxValF1: 0.8500490 --CurValF1: 0.8500490 --Patience: 02 --F1 improved: 0.8500490\n",
      "Epoch: 020 --LR: 8e-06 --MaxValF1: 0.8500490 --CurValF1: 0.8499353 --Patience: 01\n",
      "Epoch: 021 --LR: 2e-06 --MaxValF1: 0.8500490 --CurValF1: 0.8498382 --Patience: 02\n",
      "Epoch: 022 --LR: 7e-07 --MaxValF1: 0.8500490 --CurValF1: 0.8496605 --Patience: 03\n",
      "Epoch: 023 --LR: 2e-07 --MaxValF1: 0.8500490 --CurValF1: 0.8499353 --Patience: 04\n",
      "Epoch: 024 --LR: 7e-08 --MaxValF1: 0.8500490 --CurValF1: 0.8499353 --Patience: 05\n",
      "Training stopped due to the patience parameter. --Patience: 05\n",
      "Fold: 01 out of 05 completed.\n",
      "Local System Time: 12:17 AM\n",
      "Pretrained Embeddings: 28751 hits (16720 misses)\n",
      "Epoch: 001 --LR: 1e-03 --MaxValF1: 0.7684764 --CurValF1: 0.7684764 --Patience: 01 --F1 improved: 0.7684764\n",
      "Epoch: 002 --LR: 1e-03 --MaxValF1: 0.7993068 --CurValF1: 0.7993068 --Patience: 01 --F1 improved: 0.7993068\n",
      "Epoch: 003 --LR: 1e-03 --MaxValF1: 0.8080402 --CurValF1: 0.8080402 --Patience: 01 --F1 improved: 0.8080402\n",
      "Epoch: 004 --LR: 1e-03 --MaxValF1: 0.8115099 --CurValF1: 0.8115099 --Patience: 01 --F1 improved: 0.8115099\n",
      "Epoch: 005 --LR: 1e-03 --MaxValF1: 0.8216620 --CurValF1: 0.8216620 --Patience: 01 --F1 improved: 0.8216620\n",
      "Epoch: 006 --LR: 1e-03 --MaxValF1: 0.8296972 --CurValF1: 0.8296972 --Patience: 01 --F1 improved: 0.8296972\n",
      "Epoch: 007 --LR: 1e-03 --MaxValF1: 0.8296972 --CurValF1: 0.8294017 --Patience: 01\n",
      "Epoch: 008 --LR: 3e-04 --MaxValF1: 0.8395462 --CurValF1: 0.8395462 --Patience: 02 --F1 improved: 0.8395462\n",
      "Epoch: 009 --LR: 3e-04 --MaxValF1: 0.8428202 --CurValF1: 0.8428202 --Patience: 01 --F1 improved: 0.8428202\n",
      "Epoch: 010 --LR: 3e-04 --MaxValF1: 0.8428202 --CurValF1: 0.8414432 --Patience: 01\n",
      "Epoch: 011 --LR: 9e-05 --MaxValF1: 0.8439812 --CurValF1: 0.8439812 --Patience: 02 --F1 improved: 0.8439812\n",
      "Epoch: 012 --LR: 9e-05 --MaxValF1: 0.8439812 --CurValF1: 0.8423498 --Patience: 01\n",
      "Epoch: 013 --LR: 3e-05 --MaxValF1: 0.8439812 --CurValF1: 0.8419011 --Patience: 02\n",
      "Epoch: 014 --LR: 8e-06 --MaxValF1: 0.8439812 --CurValF1: 0.8422754 --Patience: 03\n",
      "Epoch: 015 --LR: 2e-06 --MaxValF1: 0.8439812 --CurValF1: 0.8422072 --Patience: 04\n",
      "Epoch: 016 --LR: 7e-07 --MaxValF1: 0.8439812 --CurValF1: 0.8422072 --Patience: 05\n",
      "Training stopped due to the patience parameter. --Patience: 05\n",
      "Fold: 02 out of 05 completed.\n",
      "Local System Time: 12:22 AM\n",
      "Pretrained Embeddings: 28866 hits (16643 misses)\n",
      "Epoch: 001 --LR: 1e-03 --MaxValF1: 0.7637475 --CurValF1: 0.7637475 --Patience: 01 --F1 improved: 0.7637475\n",
      "Epoch: 002 --LR: 1e-03 --MaxValF1: 0.7877374 --CurValF1: 0.7877374 --Patience: 01 --F1 improved: 0.7877374\n",
      "Epoch: 003 --LR: 1e-03 --MaxValF1: 0.8092750 --CurValF1: 0.8092750 --Patience: 01 --F1 improved: 0.8092750\n",
      "Epoch: 004 --LR: 1e-03 --MaxValF1: 0.8198347 --CurValF1: 0.8198347 --Patience: 01 --F1 improved: 0.8198347\n",
      "Epoch: 005 --LR: 1e-03 --MaxValF1: 0.8355526 --CurValF1: 0.8355526 --Patience: 01 --F1 improved: 0.8355526\n",
      "Epoch: 006 --LR: 1e-03 --MaxValF1: 0.8393379 --CurValF1: 0.8393379 --Patience: 01 --F1 improved: 0.8393379\n",
      "Epoch: 007 --LR: 1e-03 --MaxValF1: 0.8393379 --CurValF1: 0.8360065 --Patience: 01\n",
      "Epoch: 008 --LR: 3e-04 --MaxValF1: 0.8412698 --CurValF1: 0.8412698 --Patience: 02 --F1 improved: 0.8412698\n",
      "Epoch: 009 --LR: 3e-04 --MaxValF1: 0.8417335 --CurValF1: 0.8417335 --Patience: 01 --F1 improved: 0.8417335\n",
      "Epoch: 010 --LR: 3e-04 --MaxValF1: 0.8434442 --CurValF1: 0.8434442 --Patience: 01 --F1 improved: 0.8434442\n",
      "Epoch: 011 --LR: 3e-04 --MaxValF1: 0.8486269 --CurValF1: 0.8486269 --Patience: 01 --F1 improved: 0.8486269\n",
      "Epoch: 012 --LR: 3e-04 --MaxValF1: 0.8486269 --CurValF1: 0.8474465 --Patience: 01\n",
      "Epoch: 013 --LR: 9e-05 --MaxValF1: 0.8498830 --CurValF1: 0.8498830 --Patience: 02 --F1 improved: 0.8498830\n",
      "Epoch: 014 --LR: 9e-05 --MaxValF1: 0.8498830 --CurValF1: 0.8495277 --Patience: 01\n",
      "Epoch: 015 --LR: 3e-05 --MaxValF1: 0.8525132 --CurValF1: 0.8525132 --Patience: 02 --F1 improved: 0.8525132\n",
      "Epoch: 016 --LR: 3e-05 --MaxValF1: 0.8525132 --CurValF1: 0.8520986 --Patience: 01\n",
      "Epoch: 017 --LR: 8e-06 --MaxValF1: 0.8525132 --CurValF1: 0.8510638 --Patience: 02\n",
      "Epoch: 018 --LR: 2e-06 --MaxValF1: 0.8525132 --CurValF1: 0.8516301 --Patience: 03\n",
      "Epoch: 019 --LR: 7e-07 --MaxValF1: 0.8525132 --CurValF1: 0.8513469 --Patience: 04\n",
      "Epoch: 020 --LR: 2e-07 --MaxValF1: 0.8525132 --CurValF1: 0.8513469 --Patience: 05\n",
      "Training stopped due to the patience parameter. --Patience: 05\n",
      "Fold: 03 out of 05 completed.\n",
      "Local System Time: 12:29 AM\n",
      "Pretrained Embeddings: 28831 hits (16686 misses)\n",
      "Epoch: 001 --LR: 1e-03 --MaxValF1: 0.7807164 --CurValF1: 0.7807164 --Patience: 01 --F1 improved: 0.7807164\n",
      "Epoch: 002 --LR: 1e-03 --MaxValF1: 0.8010993 --CurValF1: 0.8010993 --Patience: 01 --F1 improved: 0.8010993\n",
      "Epoch: 003 --LR: 1e-03 --MaxValF1: 0.8156052 --CurValF1: 0.8156052 --Patience: 01 --F1 improved: 0.8156052\n",
      "Epoch: 004 --LR: 1e-03 --MaxValF1: 0.8279253 --CurValF1: 0.8279253 --Patience: 01 --F1 improved: 0.8279253\n",
      "Epoch: 005 --LR: 1e-03 --MaxValF1: 0.8374638 --CurValF1: 0.8374638 --Patience: 01 --F1 improved: 0.8374638\n",
      "Epoch: 006 --LR: 1e-03 --MaxValF1: 0.8374638 --CurValF1: 0.8336036 --Patience: 01\n",
      "Epoch: 007 --LR: 3e-04 --MaxValF1: 0.8463826 --CurValF1: 0.8463826 --Patience: 02 --F1 improved: 0.8463826\n",
      "Epoch: 008 --LR: 3e-04 --MaxValF1: 0.8463826 --CurValF1: 0.8461035 --Patience: 01\n",
      "Epoch: 009 --LR: 9e-05 --MaxValF1: 0.8463826 --CurValF1: 0.8463295 --Patience: 02\n",
      "Epoch: 010 --LR: 3e-05 --MaxValF1: 0.8463826 --CurValF1: 0.8459770 --Patience: 03\n",
      "Epoch: 011 --LR: 8e-06 --MaxValF1: 0.8463826 --CurValF1: 0.8458122 --Patience: 04\n",
      "Epoch: 012 --LR: 2e-06 --MaxValF1: 0.8464593 --CurValF1: 0.8464593 --Patience: 05 --F1 improved: 0.8464593\n",
      "Epoch: 013 --LR: 2e-06 --MaxValF1: 0.8464593 --CurValF1: 0.8459755 --Patience: 01\n",
      "Epoch: 014 --LR: 7e-07 --MaxValF1: 0.8464593 --CurValF1: 0.8460775 --Patience: 02\n",
      "Epoch: 015 --LR: 2e-07 --MaxValF1: 0.8464593 --CurValF1: 0.8463576 --Patience: 03\n",
      "Epoch: 016 --LR: 7e-08 --MaxValF1: 0.8464593 --CurValF1: 0.8463576 --Patience: 04\n",
      "Epoch: 017 --LR: 2e-08 --MaxValF1: 0.8464593 --CurValF1: 0.8463576 --Patience: 05\n",
      "Training stopped due to the patience parameter. --Patience: 05\n",
      "Fold: 04 out of 05 completed.\n",
      "Local System Time: 12:35 AM\n",
      "Pretrained Embeddings: 28812 hits (16602 misses)\n",
      "Epoch: 001 --LR: 1e-03 --MaxValF1: 0.7748656 --CurValF1: 0.7748656 --Patience: 01 --F1 improved: 0.7748656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002 --LR: 1e-03 --MaxValF1: 0.7938420 --CurValF1: 0.7938420 --Patience: 01 --F1 improved: 0.7938420\n",
      "Epoch: 003 --LR: 1e-03 --MaxValF1: 0.8079722 --CurValF1: 0.8079722 --Patience: 01 --F1 improved: 0.8079722\n",
      "Epoch: 004 --LR: 1e-03 --MaxValF1: 0.8205298 --CurValF1: 0.8205298 --Patience: 01 --F1 improved: 0.8205298\n",
      "Epoch: 005 --LR: 1e-03 --MaxValF1: 0.8253436 --CurValF1: 0.8253436 --Patience: 01 --F1 improved: 0.8253436\n",
      "Epoch: 006 --LR: 1e-03 --MaxValF1: 0.8357810 --CurValF1: 0.8357810 --Patience: 01 --F1 improved: 0.8357810\n",
      "Epoch: 007 --LR: 1e-03 --MaxValF1: 0.8368892 --CurValF1: 0.8368892 --Patience: 01 --F1 improved: 0.8368892\n",
      "Epoch: 008 --LR: 1e-03 --MaxValF1: 0.8390551 --CurValF1: 0.8390551 --Patience: 01 --F1 improved: 0.8390551\n",
      "Epoch: 009 --LR: 1e-03 --MaxValF1: 0.8414834 --CurValF1: 0.8414834 --Patience: 01 --F1 improved: 0.8414834\n",
      "Epoch: 010 --LR: 1e-03 --MaxValF1: 0.8414834 --CurValF1: 0.8374384 --Patience: 01\n",
      "Epoch: 011 --LR: 3e-04 --MaxValF1: 0.8497882 --CurValF1: 0.8497882 --Patience: 02 --F1 improved: 0.8497882\n",
      "Epoch: 012 --LR: 3e-04 --MaxValF1: 0.8527787 --CurValF1: 0.8527787 --Patience: 01 --F1 improved: 0.8527787\n",
      "Epoch: 013 --LR: 3e-04 --MaxValF1: 0.8527787 --CurValF1: 0.8508655 --Patience: 01\n",
      "Epoch: 014 --LR: 9e-05 --MaxValF1: 0.8527787 --CurValF1: 0.8500813 --Patience: 02\n",
      "Epoch: 015 --LR: 3e-05 --MaxValF1: 0.8527787 --CurValF1: 0.8513952 --Patience: 03\n",
      "Epoch: 016 --LR: 8e-06 --MaxValF1: 0.8527787 --CurValF1: 0.8512987 --Patience: 04\n",
      "Epoch: 017 --LR: 2e-06 --MaxValF1: 0.8527787 --CurValF1: 0.8511327 --Patience: 05\n",
      "Training stopped due to the patience parameter. --Patience: 05\n",
      "Fold: 05 out of 05 completed.\n",
      "Local System Time: 12:41 AM\n",
      "Total runtime: 0 hrs 32 mins 57.03 secs\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings( 'ignore' )\n",
    "start_time = time.time()\n",
    "print( 'Local System Time: {}'.format( time.strftime( \"%I:%M %p\", time.localtime() ) ) )\n",
    "\n",
    "valaccuracy, valprecision, valrecall, valf1, valcm = [], [], [], [], []\n",
    "testaccuracy, testprecision, testrecall, testf1, testcm = [], [], [], [], []\n",
    "com_text, com_label, com_predicted, com_prob = [], [], [], []\n",
    "com_indices = []\n",
    "hist = {}\n",
    "\n",
    "fold = 1\n",
    "for train_index, test_index in skf.split( df[ xcolumn ], df[ ycolumn ] ):\n",
    "    # seprating train and test sets\n",
    "    xtrain = df.loc[ train_index ][ xcolumn ].values\n",
    "    xtest = df.loc[ test_index ][ xcolumn ].values\n",
    "    \n",
    "    ytrain = df.loc[ train_index ][ ycolumn ].values\n",
    "    ytest = df.loc[ test_index ][ ycolumn ].values\n",
    "    \n",
    "    # splitting train and validation sets\n",
    "    xtrain, xval, ytrain, yval = train_test_split( xtrain, ytrain, test_size=0.15, random_state=0 )\n",
    "    \n",
    "    # tokenization with keras tokenizer\n",
    "    tokenizer = text.Tokenizer(  )\n",
    "    tokenizer.fit_on_texts( np.append( xtrain, xval ) )\n",
    "\n",
    "    xtrain = tokenizer.texts_to_sequences( xtrain )\n",
    "    xval = tokenizer.texts_to_sequences( xval )\n",
    "    xtest = tokenizer.texts_to_sequences( xtest )\n",
    "    \n",
    "    # pad the tokenized sequences\n",
    "    xtrain = sequence.pad_sequences( xtrain, maxlen=MAX_LEN )\n",
    "    xval = sequence.pad_sequences( xval, maxlen=MAX_LEN )\n",
    "    xtest = sequence.pad_sequences( xtest, maxlen=MAX_LEN )\n",
    "    \n",
    "    # check if pre-trained word embeddings flag is true\n",
    "    if PRE_TRAINED_FLAG == True:\n",
    "        EMBEDDING_MATRIX = get_vectors(\n",
    "            tokenizer=tokenizer, emb_mean=emb_mean, emb_std=emb_std, embed_size=EMBED_SIZE\n",
    "        )\n",
    "    \n",
    "    # define a model\n",
    "    EMBED_INP_SIZE = len( tokenizer.word_index ) + 1\n",
    "    model = All_UT_Models.BLSTM(\n",
    "        max_len=MAX_LEN, embed_inp=EMBED_INP_SIZE, embed_size=EMBED_SIZE,\n",
    "        embedding_matrix=EMBEDDING_MATRIX, embed_trainable=EMBED_TRAINABLE,\n",
    "        spdrpt=SPDRPT, drpt=DRPT, emb_weights_init=EMB_WEIGHTS_INIT,\n",
    "        fc_weights_init=FC_WEIGHTS_INIT, fc_act=FC_ACT, optimizer=OPTIMIZER, lstm_units=LSTM_UNITS\n",
    "    )\n",
    "    \n",
    "    # save model summaries and model architecture diagrams in the first fold only\n",
    "    if fold == 1:\n",
    "        plot_model( model=model, to_file='{}/{}.png'.format( modelsummaries, modelname ), show_shapes=False )\n",
    "        \n",
    "        with open( '{}/{}.txt'.format( modelsummaries, modelname ), 'w' ) as s:\n",
    "            with redirect_stdout( s ):\n",
    "                model.summary()\n",
    "    \n",
    "    K.set_value( model.optimizer.lr, LR_RATE )\n",
    "    \n",
    "    # train the model with callbacks for early stopping\n",
    "    f1callback = UT_Utils.F1_score_callback(\n",
    "        val_data=( xval, yval ), filepath=modelpath + modelname + str( fold ),\n",
    "        patience=PATIENCE, decay=DECAY, decay_rate=DECAY_RATE, decay_after=DECAY_AFTER\n",
    "    )\n",
    "    histmodel = model.fit(\n",
    "        xtrain, ytrain, batch_size=BATCH, epochs=NEPOCHS, verbose=0, callbacks=[ f1callback ]\n",
    "    )\n",
    "    \n",
    "    # save history of all folds\n",
    "    hist[ 'fold' + str( fold ) ] = histmodel.history.copy()\n",
    "    \n",
    "    # delete trained model object\n",
    "    del model, histmodel, f1callback\n",
    "    EMBEDDING_MATRIX = []\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    # delete pre trained embeddings from the memory\n",
    "    if ( PRE_TRAINED_FLAG == True ) and ( fold == skf.n_splits ):\n",
    "        del EMBEDDINGS_INDEX\n",
    "        gc.collect()\n",
    "    \n",
    "    # load saved model\n",
    "    loaded_model = load_model( modelpath + modelname + str( fold ) )\n",
    "    \n",
    "    # get predictions (probabilities) for validation and test sets respectively\n",
    "    valpredictions = loaded_model.predict( xval, verbose=0, batch_size=BATCH )\n",
    "    testpredictions = loaded_model.predict( xtest, verbose=0, batch_size=BATCH )\n",
    "    \n",
    "    # delete loaded model\n",
    "    del loaded_model\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    # optimizer threshold on validation set\n",
    "    threshold = UT_Utils.optimize_threshold( yval, valpredictions )\n",
    "    \n",
    "    # save accuracy, precision, recall, f1 and confusion matrices\n",
    "    vallabels = ( valpredictions >= threshold ).astype( 'int32' )\n",
    "    testlabels = ( testpredictions >= threshold ).astype( 'int32' )\n",
    "    \n",
    "    valaccuracy.append( accuracy_score( yval, vallabels ) )\n",
    "    valprecision.append( precision_score( yval, vallabels ) )\n",
    "    valrecall.append( recall_score( yval, vallabels ) )\n",
    "    valf1.append( f1_score( yval, vallabels ) )\n",
    "    valcm.append( confusion_matrix( yval, vallabels ) )    \n",
    "    \n",
    "    testaccuracy.append( accuracy_score( ytest, testlabels ) )\n",
    "    testprecision.append( precision_score( ytest, testlabels ) )\n",
    "    testrecall.append( recall_score( ytest, testlabels ) )\n",
    "    testf1.append( f1_score( ytest, testlabels ) )\n",
    "    testcm.append( confusion_matrix( ytest, testlabels ) )\n",
    "    \n",
    "    # save for future analysis and ensemble\n",
    "    com_indices.extend( test_index.tolist() )\n",
    "    com_text.extend( df.loc[ test_index ][ xcolumn ] )\n",
    "    com_label.extend( df.loc[ test_index ][ ycolumn ].tolist() )\n",
    "    com_predicted.extend( testlabels[:,0].tolist() )\n",
    "    com_prob.extend( testpredictions[:,0].tolist() )\n",
    "    \n",
    "    print( 'Fold: {:02d} out of {:02d} completed.'.format( fold, skf.get_n_splits() ) )\n",
    "    print( 'Local System Time: {}'.format( time.strftime( \"%I:%M %p\", time.localtime() ) ) )\n",
    "    \n",
    "    fold = fold + 1\n",
    "time_took = time.time() - start_time\n",
    "print( f\"Total runtime: { hms_string( time_took ) }\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy\n",
      "['0.9474', '0.9469', '0.9489', '0.9469', '0.9481'] 0.9476468567502577 +- 0.0007913501680142422 \n",
      "\n",
      "Validation Precision\n",
      "['0.8526', '0.8746', '0.8680', '0.8624', '0.8531'] 0.8621261087750882 +- 0.008524480253182426 \n",
      "\n",
      "Validation Recall\n",
      "['0.8476', '0.8155', '0.8376', '0.8311', '0.8525'] 0.8368278398977296 +- 0.01304592281595098 \n",
      "\n",
      "Validation F1\n",
      "['0.8500', '0.8440', '0.8525', '0.8465', '0.8528'] 0.8491562760517729 +- 0.00344218851964163\n"
     ]
    }
   ],
   "source": [
    "print( 'Validation Accuracy' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in valaccuracy ], np.mean( valaccuracy ), '+-', np.std( valaccuracy ), '\\n' )\n",
    "\n",
    "print( 'Validation Precision' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in valprecision ], np.mean( valprecision ), '+-', np.std( valprecision ), '\\n' )\n",
    "\n",
    "print( 'Validation Recall' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in valrecall ], np.mean( valrecall ), '+-', np.std( valrecall ), '\\n' )\n",
    "\n",
    "print( 'Validation F1' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in valf1 ], np.mean( valf1 ), '+-', np.std( valf1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1301  234]\n",
      " [ 225 6973]] \n",
      "\n",
      "[[1255  284]\n",
      " [ 180 7014]] \n",
      "\n",
      "[[1289  250]\n",
      " [ 196 6998]] \n",
      "\n",
      "[[1279  260]\n",
      " [ 204 6990]] \n",
      "\n",
      "[[1312  227]\n",
      " [ 226 6968]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in valcm:\n",
    "    print( np.rot90(np.rot90(c)), '\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy\n",
      "['0.9492', '0.9469', '0.9483', '0.9436', '0.9489'] 0.9473691197146513 +- 0.0020458045660989204 \n",
      "\n",
      "Test Precision\n",
      "['0.8824', '0.8802', '0.8733', '0.8721', '0.8791'] 0.8774139828039436 +- 0.0040304495341339725 \n",
      "\n",
      "Test Recall\n",
      "['0.8279', '0.8160', '0.8339', '0.8045', '0.8301'] 0.8224785114066613 +- 0.010789655134905958 \n",
      "\n",
      "Test F1\n",
      "['0.8543', '0.8469', '0.8531', '0.8369', '0.8539'] 0.8490257149749493 +- 0.00661113254651028\n"
     ]
    }
   ],
   "source": [
    "print( 'Test Accuracy' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in testaccuracy ], np.mean( testaccuracy ), '+-', np.std( testaccuracy ), '\\n' )\n",
    "\n",
    "print( 'Test Precision' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in testprecision ], np.mean( testprecision ), '+-', np.std( testprecision ), '\\n' )\n",
    "\n",
    "print( 'Test Recall' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in testrecall ], np.mean( testrecall ), '+-', np.std( testrecall ), '\\n' )\n",
    "\n",
    "print( 'Test F1' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in testf1 ], np.mean( testf1 ), '+-', np.std( testf1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2169   451]\n",
      " [  289 11646]] \n",
      "\n",
      "[[ 2138   482]\n",
      " [  291 11643]] \n",
      "\n",
      "[[ 2184   435]\n",
      " [  317 11618]] \n",
      "\n",
      "[[ 2107   512]\n",
      " [  309 11626]] \n",
      "\n",
      "[[ 2174   445]\n",
      " [  299 11636]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in testcm:\n",
    "    print( np.rot90(np.rot90(c)), '\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open( '{}/ResultsMain.csv'.format( modelresults ), mode='a' )\n",
    "file.write( modelname )\n",
    "file.write( ',' )\n",
    "file.write( str(np.mean( testaccuracy ))[:7] + '+-' + str(np.std( testaccuracy ))[:6] )\n",
    "file.write( ',' )\n",
    "file.write( str(np.mean( testprecision ))[:7] + '+-' + str(np.std( testprecision ))[:6] )\n",
    "file.write( ',' )\n",
    "file.write( str(np.mean( testrecall ))[:7] + '+-' + str(np.std( testrecall ))[:6] )\n",
    "file.write( ',' )\n",
    "file.write( str(np.mean( testf1 ))[:7] + '+-' + str(np.std( testf1 ))[:6] )\n",
    "file.write( '\\n' )\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72771, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfPredictions = pd.DataFrame(  )\n",
    "dfPredictions[ 'comment_indices' ] = com_indices\n",
    "#dfPredictions[ 'comment_text' ] = com_text #comment text\n",
    "dfPredictions[ 'comment_label' ] = com_label\n",
    "dfPredictions[ 'comment_predicted' ] = com_predicted\n",
    "dfPredictions[ 'comment_prob' ] = com_prob\n",
    "dfPredictions.to_csv( '{}/{}.csv'.format( modelresults, modelname ), index=False )\n",
    "dfPredictions.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2.8",
   "language": "python",
   "name": "tf_2.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
